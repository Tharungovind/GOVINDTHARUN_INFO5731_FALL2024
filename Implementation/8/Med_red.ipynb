{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9Jq5MXCfd0_",
        "outputId": "e65ea567-aa5d-4cfb-e491-fdd63ced30bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1 (https://huggingface.co/facebook/bart-large-mnli).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Zero-Shot Correctness Score: 0.30413214010851725\n",
            "Average Zero-Shot Completeness Score: 0.4247873084885733\n",
            "Average Zero-Shot Comprehensiveness Score: 0.30719225321497234\n",
            "Average Few-Shot Correctness Score: 0.7999999999999999\n",
            "Average Few-Shot Completeness Score: 0.7999999999999999\n",
            "Average Few-Shot Comprehensiveness Score: 0.7999999999999999\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import re\n",
        "\n",
        "# Load the Hugging Face pipeline for Zero-Shot classification\n",
        "zero_shot_classifier = pipeline(\"zero-shot-classification\")\n",
        "\n",
        "# Function to get Hugging Face response for Zero-Shot classification\n",
        "def get_huggingface_response(prompt, model=\"facebook/bart-large-mnli\", temperature=0.7):\n",
        "    result = zero_shot_classifier(prompt, candidate_labels=[\"correct\", \"incorrect\", \"complete\", \"incomplete\", \"comprehensive\", \"incomplete\"])\n",
        "    return result['scores'][0]  # Return the score for the most likely label\n",
        "\n",
        "# Function to compute correctness score using Hugging Face (Zero-Shot and Few-Shot)\n",
        "def compute_correctness_score(description, disease_term, model=\"facebook/bart-large-mnli\"):\n",
        "    # Zero-shot Prompt: Hugging Face should evaluate how similar the disease term is to the description\n",
        "    zero_shot_prompt = f\"Evaluate the correctness of the disease description in relation to the disease term. The description: '{description}' and the disease term: '{disease_term}'. Rate the correctness on a scale from 0 to 1.\"\n",
        "\n",
        "    correctness_score_zero_shot = get_huggingface_response(zero_shot_prompt, model)\n",
        "    return correctness_score_zero_shot\n",
        "\n",
        "\n",
        "# Function to compute completeness score using Hugging Face (Zero-Shot and Few-Shot)\n",
        "def compute_completeness_score(description, model=\"facebook/bart-large-mnli\"):\n",
        "    # Zero-shot Prompt: Hugging Face should evaluate if the description covers all aspects of the disease\n",
        "    zero_shot_prompt = f\"Is the following disease description complete? Does it include key symptoms, diagnostic information, and relevant details about the disease? Describe any missing aspects. The description: '{description}'\"\n",
        "\n",
        "    completeness_score_zero_shot = get_huggingface_response(zero_shot_prompt, model)\n",
        "    return completeness_score_zero_shot\n",
        "\n",
        "\n",
        "# Function to compute comprehensiveness score using Hugging Face (Zero-Shot and Few-Shot)\n",
        "def compute_comprehensiveness_score(description, model=\"facebook/bart-large-mnli\"):\n",
        "    # Zero-shot Prompt: Hugging Face should evaluate the comprehensiveness of the description\n",
        "    zero_shot_prompt = f\"Is the following disease description comprehensive? Does it include symptoms, diagnostic tests, treatments, and all relevant details? Rate the comprehensiveness on a scale from 0 to 1. The description: '{description}'\"\n",
        "\n",
        "    comprehensiveness_score_zero_shot = get_huggingface_response(zero_shot_prompt, model)\n",
        "    return comprehensiveness_score_zero_shot\n",
        "\n",
        "\n",
        "# Few-shot Prompting Example for Correctness, Completeness, and Comprehensiveness\n",
        "def few_shot_prompt(prompt, model=\"gpt2\"):\n",
        "    # Example 1: Correctness\n",
        "    example_1 = \"Description: 'A condition characterized by joint pain and stiffness. Often causes inflammation.' Disease Term: 'Arthritis'. Correctness: 0.8\"\n",
        "    example_2 = \"Description: 'A disorder affecting the nervous system with symptoms of paralysis and muscle weakness.' Disease Term: 'Multiple Sclerosis'. Correctness: 0.9\"\n",
        "\n",
        "    # Example 2: Completeness\n",
        "    example_3 = \"Description: 'A disease caused by the flu virus, characterized by fever, chills, sore throat, and fatigue.' Completeness: 0.8\"\n",
        "    example_4 = \"Description: 'A chronic condition involving high blood sugar levels and insulin resistance.' Completeness: 0.9\"\n",
        "\n",
        "    # Example 3: Comprehensiveness\n",
        "    example_5 = \"Description: 'A condition that involves an autoimmune response that attacks the joints.' Comprehensiveness: 0.7\"\n",
        "    example_6 = \"Description: 'A mental health condition characterized by persistent sadness, feelings of hopelessness, and a lack of interest in daily activities.' Comprehensiveness: 0.85\"\n",
        "\n",
        "    # Combine the examples and prompt\n",
        "    few_shot_examples = \"\\n\".join([example_1, example_2, example_3, example_4, example_5, example_6])\n",
        "\n",
        "    prompt_with_few_shot = f\"{few_shot_examples}\\n\\n{prompt}\"\n",
        "\n",
        "    # Use the GPT-2 model from Hugging Face for Few-Shot Prompting\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model)\n",
        "\n",
        "    inputs = tokenizer(prompt_with_few_shot, return_tensors=\"pt\")\n",
        "    outputs = model.generate(**inputs, max_length=500, num_return_sequences=1, temperature=0.7)\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract the numerical score from the response using regex (e.g., for correctness)\n",
        "    match = re.search(r\"(\\d\\.\\d+)\", response)\n",
        "    if match:\n",
        "        return float(match.group(1))  # Return the numerical score\n",
        "    else:\n",
        "        return 0.0  # Default to 0.0 if no match is found\n",
        "\n",
        "\n",
        "# Read the CSV file using pandas (replace 'your_file.csv' with the actual file path)\n",
        "csv_file_path = 'MedRed_AMT_labels.csv'  # Replace this with your actual CSV file path\n",
        "df = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Initialize score accumulators for both Zero-Shot and Few-Shot approaches\n",
        "zero_shot_correctness_scores = []\n",
        "zero_shot_completeness_scores = []\n",
        "zero_shot_comprehensiveness_scores = []\n",
        "\n",
        "few_shot_correctness_scores = []\n",
        "few_shot_completeness_scores = []\n",
        "few_shot_comprehensiveness_scores = []\n",
        "\n",
        "# Evaluate the first few rows of the CSV file\n",
        "for i in range(7):\n",
        "    description = df.iloc[i]['post']  # Adjust the column name accordingly\n",
        "    disease_term = df.iloc[i]['subreddit']  # Adjust the column name accordingly\n",
        "\n",
        "    # Compute the scores using Zero-Shot approach\n",
        "    correctness_score_zero_shot = compute_correctness_score(description, disease_term)\n",
        "    completeness_score_zero_shot = compute_completeness_score(description)\n",
        "    comprehensiveness_score_zero_shot = compute_comprehensiveness_score(description)\n",
        "\n",
        "    zero_shot_correctness_scores.append(correctness_score_zero_shot)\n",
        "    zero_shot_completeness_scores.append(completeness_score_zero_shot)\n",
        "    zero_shot_comprehensiveness_scores.append(comprehensiveness_score_zero_shot)\n",
        "\n",
        "    # Compute the scores using Few-Shot approach\n",
        "    few_shot_correctness_prompt = f\"Evaluate the correctness of the disease description: '{description}' with the disease term: '{disease_term}'. Rate the correctness from 0 to 1.\"\n",
        "    few_shot_completeness_prompt = f\"Is the following disease description complete? Rate the completeness from 0 to 1. The description: '{description}'\"\n",
        "    few_shot_comprehensiveness_prompt = f\"Evaluate if the description includes symptoms, diagnostic tests, and treatments. Rate the comprehensiveness from 0 to 1. The description: '{description}'\"\n",
        "\n",
        "    correctness_score_few_shot = few_shot_prompt(few_shot_correctness_prompt)\n",
        "    completeness_score_few_shot = few_shot_prompt(few_shot_completeness_prompt)\n",
        "    comprehensiveness_score_few_shot = few_shot_prompt(few_shot_comprehensiveness_prompt)\n",
        "\n",
        "    few_shot_correctness_scores.append(correctness_score_few_shot)\n",
        "    few_shot_completeness_scores.append(completeness_score_few_shot)\n",
        "    few_shot_comprehensiveness_scores.append(comprehensiveness_score_few_shot)\n",
        "\n",
        "# Calculate the average scores for Zero-Shot and Few-Shot approaches\n",
        "avg_zero_shot_correctness = sum(zero_shot_correctness_scores) / len(zero_shot_correctness_scores)\n",
        "avg_zero_shot_completeness = sum(zero_shot_completeness_scores) / len(zero_shot_completeness_scores)\n",
        "avg_zero_shot_comprehensiveness = sum(zero_shot_comprehensiveness_scores) / len(zero_shot_comprehensiveness_scores)\n",
        "\n",
        "avg_few_shot_correctness = sum(few_shot_correctness_scores) / len(few_shot_correctness_scores)\n",
        "avg_few_shot_completeness = sum(few_shot_completeness_scores) / len(few_shot_completeness_scores)\n",
        "avg_few_shot_comprehensiveness = sum(few_shot_comprehensiveness_scores) / len(few_shot_comprehensiveness_scores)\n",
        "\n",
        "# Print out the average scores for both Zero-Shot and Few-Shot approaches\n",
        "print(f\"Average Zero-Shot Correctness Score: {avg_zero_shot_correctness}\")\n",
        "print(f\"Average Zero-Shot Completeness Score: {avg_zero_shot_completeness}\")\n",
        "print(f\"Average Zero-Shot Comprehensiveness Score: {avg_zero_shot_comprehensiveness}\")\n",
        "print(f\"Average Few-Shot Correctness Score: {avg_few_shot_correctness}\")\n",
        "print(f\"Average Few-Shot Completeness Score: {avg_few_shot_completeness}\")\n",
        "print(f\"Average Few-Shot Comprehensiveness Score: {avg_few_shot_comprehensiveness}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Function to extract unique pairs of disease and description\n",
        "def get_unique_pairs(df):\n",
        "    unique_pairs = set()  # Store unique pairs in a set to avoid duplicates\n",
        "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing Entries\"):\n",
        "        pair = (row['subreddit'], row['Post'])\n",
        "        unique_pairs.add(pair)\n",
        "    return unique_pairs\n",
        "\n",
        "# Function to calculate uniqueness score for a dataset\n",
        "def calculate_uniqueness_score(df):\n",
        "    unique_pairs = get_unique_pairs(df)\n",
        "    total_entries = len(df)\n",
        "    unique_pairs_count = len(unique_pairs)\n",
        "    uniqueness_score = unique_pairs_count / total_entries\n",
        "    return uniqueness_score\n",
        "\n",
        "# Load the dataset\n",
        "dataset_path = 'first_100_records.csv'\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "# Calculate and print the uniqueness score for the dataset\n",
        "uniqueness_score = calculate_uniqueness_score(df)\n",
        "print(f\"Uniqueness Score {uniqueness_score:.4f}\")\n"
      ],
      "metadata": {
        "id": "jPL7E2G8ipx8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}