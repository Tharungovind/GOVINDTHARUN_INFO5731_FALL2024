{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PC6CKIaxSjW",
        "outputId": "423c5792-ddbf-4926-9171-1bd32fec4309"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'train-00000-of-00001.parquet' has been converted to CSV format and saved as 'output_file.csv'.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Input and output file paths\n",
        "input_parquet_file = 'train-00000-of-00001.parquet'\n",
        "output_csv_file = 'output_file.csv'\n",
        "\n",
        "# Read the Parquet file into a Pandas DataFrame\n",
        "df = pd.read_parquet(input_parquet_file)\n",
        "\n",
        "# Save the DataFrame as a CSV file\n",
        "df.to_csv(output_csv_file, index=False)\n",
        "\n",
        "print(f\"File '{input_parquet_file}' has been converted to CSV format and saved as '{output_csv_file}'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import re\n",
        "\n",
        "# Load the Hugging Face pipeline for Zero-Shot classification\n",
        "zero_shot_classifier = pipeline(\"zero-shot-classification\")\n",
        "\n",
        "# Function to get Hugging Face response for Zero-Shot classification\n",
        "def get_huggingface_response(prompt, model=\"facebook/bart-large-mnli\", temperature=0.7):\n",
        "    result = zero_shot_classifier(prompt, candidate_labels=[\"correct\", \"incorrect\", \"complete\", \"incomplete\", \"comprehensive\", \"incomplete\"])\n",
        "    return result['scores'][0]  # Return the score for the most likely label\n",
        "\n",
        "# Function to compute correctness score using Hugging Face (Zero-Shot and Few-Shot)\n",
        "def compute_correctness_score(description, disease_term, model=\"facebook/bart-large-mnli\"):\n",
        "    # Zero-shot Prompt: Hugging Face should evaluate how similar the disease term is to the description\n",
        "    zero_shot_prompt = f\"Evaluate the correctness of the disease description in relation to the disease term. The description: '{description}' and the disease term: '{disease_term}'. Rate the correctness on a scale from 0 to 1.\"\n",
        "\n",
        "    correctness_score_zero_shot = get_huggingface_response(zero_shot_prompt, model)\n",
        "    return correctness_score_zero_shot\n",
        "\n",
        "\n",
        "# Function to compute completeness score using Hugging Face (Zero-Shot and Few-Shot)\n",
        "def compute_completeness_score(description, model=\"facebook/bart-large-mnli\"):\n",
        "    # Zero-shot Prompt: Hugging Face should evaluate if the description covers all aspects of the disease\n",
        "    zero_shot_prompt = f\"Is the following disease description complete? Does it include key symptoms, diagnostic information, and relevant details about the disease? Describe any missing aspects. The description: '{description}'\"\n",
        "\n",
        "    completeness_score_zero_shot = get_huggingface_response(zero_shot_prompt, model)\n",
        "    return completeness_score_zero_shot\n",
        "\n",
        "\n",
        "# Function to compute comprehensiveness score using Hugging Face (Zero-Shot and Few-Shot)\n",
        "def compute_comprehensiveness_score(description, model=\"facebook/bart-large-mnli\"):\n",
        "    # Zero-shot Prompt: Hugging Face should evaluate the comprehensiveness of the description\n",
        "    zero_shot_prompt = f\"Is the following disease description comprehensive? Does it include symptoms, diagnostic tests, treatments, and all relevant details? Rate the comprehensiveness on a scale from 0 to 1. The description: '{description}'\"\n",
        "\n",
        "    comprehensiveness_score_zero_shot = get_huggingface_response(zero_shot_prompt, model)\n",
        "    return comprehensiveness_score_zero_shot\n",
        "\n",
        "\n",
        "# Few-shot Prompting Example for Correctness, Completeness, and Comprehensiveness\n",
        "def few_shot_prompt(prompt, model=\"gpt2\"):\n",
        "    # Example 1: Correctness\n",
        "    example_1 = \"Description: 'A condition characterized by joint pain and stiffness. Often causes inflammation.' Disease Term: 'Arthritis'. Correctness: 0.8\"\n",
        "    example_2 = \"Description: 'A disorder affecting the nervous system with symptoms of paralysis and muscle weakness.' Disease Term: 'Multiple Sclerosis'. Correctness: 0.9\"\n",
        "\n",
        "    # Example 2: Completeness\n",
        "    example_3 = \"Description: 'A disease caused by the flu virus, characterized by fever, chills, sore throat, and fatigue.' Completeness: 0.8\"\n",
        "    example_4 = \"Description: 'A chronic condition involving high blood sugar levels and insulin resistance.' Completeness: 0.9\"\n",
        "\n",
        "    # Example 3: Comprehensiveness\n",
        "    example_5 = \"Description: 'A condition that involves an autoimmune response that attacks the joints.' Comprehensiveness: 0.7\"\n",
        "    example_6 = \"Description: 'A mental health condition characterized by persistent sadness, feelings of hopelessness, and a lack of interest in daily activities.' Comprehensiveness: 0.85\"\n",
        "\n",
        "    # Combine the examples and prompt\n",
        "    few_shot_examples = \"\\n\".join([example_1, example_2, example_3, example_4, example_5, example_6])\n",
        "\n",
        "    prompt_with_few_shot = f\"{few_shot_examples}\\n\\n{prompt}\"\n",
        "\n",
        "    # Use the GPT-2 model from Hugging Face for Few-Shot Prompting\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model)\n",
        "\n",
        "    inputs = tokenizer(prompt_with_few_shot, return_tensors=\"pt\")\n",
        "    outputs = model.generate(**inputs, max_length=400, num_return_sequences=1, temperature=0.7)\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract the numerical score from the response\n",
        "    match = re.search(r\"(\\d+\\.\\d+)\", response)\n",
        "    if match:\n",
        "        return float(match.group(1))  # Return the extracted score\n",
        "    else:\n",
        "        return 0.0  # Return a default score in case no score is found\n",
        "\n",
        "\n",
        "# Read the CSV file using pandas (replace 'your_file.csv' with the actual file path)\n",
        "csv_file_path = 'output_file.csv'  # Replace this with your actual CSV file path\n",
        "df = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Initialize variables to accumulate scores for averaging\n",
        "zero_shot_correctness_total = 0\n",
        "zero_shot_completeness_total = 0\n",
        "zero_shot_comprehensiveness_total = 0\n",
        "\n",
        "few_shot_correctness_total = 0\n",
        "few_shot_completeness_total = 0\n",
        "few_shot_comprehensiveness_total = 0\n",
        "\n",
        "# Evaluate the first few rows of the CSV file\n",
        "num_rows = 7\n",
        "for i in range(num_rows):\n",
        "    description = df.iloc[i]['text']  # Adjust the column name accordingly\n",
        "    disease_term = df.iloc[i]['label']  # Adjust the column name accordingly\n",
        "\n",
        "    # Compute the scores using Zero-Shot and Few-Shot approaches\n",
        "\n",
        "    # Zero-Shot Approach\n",
        "    correctness_score_zero_shot = compute_correctness_score(description, disease_term)\n",
        "    completeness_score_zero_shot = compute_completeness_score(description)\n",
        "    comprehensiveness_score_zero_shot = compute_comprehensiveness_score(description)\n",
        "\n",
        "    zero_shot_correctness_total += correctness_score_zero_shot\n",
        "    zero_shot_completeness_total += completeness_score_zero_shot\n",
        "    zero_shot_comprehensiveness_total += comprehensiveness_score_zero_shot\n",
        "\n",
        "    print(f\"Row {i+1} - Zero-Shot Correctness Score: {correctness_score_zero_shot}\")\n",
        "    print(f\"Row {i+1} - Zero-Shot Completeness Score: {completeness_score_zero_shot}\")\n",
        "    print(f\"Row {i+1} - Zero-Shot Comprehensiveness Score: {comprehensiveness_score_zero_shot}\")\n",
        "\n",
        "    # Few-Shot Approach\n",
        "    few_shot_correctness_prompt = f\"Evaluate the correctness of the disease description: '{description}' with the disease term: '{disease_term}'. Rate the correctness from 0 to 1.\"\n",
        "    few_shot_completeness_prompt = f\"Is the following disease description complete? Rate the completeness from 0 to 1. The description: '{description}'\"\n",
        "    few_shot_comprehensiveness_prompt = f\"Evaluate if the description includes symptoms, diagnostic tests, and treatments. Rate the comprehensiveness from 0 to 1. The description: '{description}'\"\n",
        "\n",
        "    correctness_score_few_shot = few_shot_prompt(few_shot_correctness_prompt)\n",
        "    completeness_score_few_shot = few_shot_prompt(few_shot_completeness_prompt)\n",
        "    comprehensiveness_score_few_shot = few_shot_prompt(few_shot_comprehensiveness_prompt)\n",
        "\n",
        "    few_shot_correctness_total += correctness_score_few_shot\n",
        "    few_shot_completeness_total += completeness_score_few_shot\n",
        "    few_shot_comprehensiveness_total += comprehensiveness_score_few_shot\n",
        "\n",
        "    print(f\"Row {i+1} - Few-Shot Correctness Score: {correctness_score_few_shot}\")\n",
        "    print(f\"Row {i+1} - Few-Shot Completeness Score: {completeness_score_few_shot}\")\n",
        "    print(f\"Row {i+1} - Few-Shot Comprehensiveness Score: {comprehensiveness_score_few_shot}\")\n",
        "\n",
        "# Calculate and print the average scores for Zero-Shot and Few-Shot\n",
        "zero_shot_correctness_avg = zero_shot_correctness_total / num_rows\n",
        "zero_shot_completeness_avg = zero_shot_completeness_total / num_rows\n",
        "zero_shot_comprehensiveness_avg = zero_shot_comprehensiveness_total / num_rows\n",
        "\n",
        "few_shot_correctness_avg = few_shot_correctness_total / num_rows\n",
        "few_shot_completeness_avg = few_shot_completeness_total / num_rows\n",
        "few_shot_comprehensiveness_avg = few_shot_comprehensiveness_total / num_rows\n",
        "\n",
        "print(f\"\\nZero-Shot Average Correctness Score: {zero_shot_correctness_avg}\")\n",
        "print(f\"Zero-Shot Average Completeness Score: {zero_shot_completeness_avg}\")\n",
        "print(f\"Zero-Shot Average Comprehensiveness Score: {zero_shot_comprehensiveness_avg}\")\n",
        "\n",
        "print(f\"\\nFew-Shot Average Correctness Score: {few_shot_correctness_avg}\")\n",
        "print(f\"Few-Shot Average Completeness Score: {few_shot_completeness_avg}\")\n",
        "print(f\"Few-Shot Average Comprehensiveness Score: {few_shot_comprehensiveness_avg}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwG9_ZX3xmca",
        "outputId": "6e68d60d-bb61-4d1f-9b08-0a8a2a6e80b7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1 (https://huggingface.co/facebook/bart-large-mnli).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 1 - Zero-Shot Correctness Score: 0.2984800934791565\n",
            "Row 1 - Zero-Shot Completeness Score: 0.4353615939617157\n",
            "Row 1 - Zero-Shot Comprehensiveness Score: 0.32834190130233765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 1 - Few-Shot Correctness Score: 0.8\n",
            "Row 1 - Few-Shot Completeness Score: 0.8\n",
            "Row 1 - Few-Shot Comprehensiveness Score: 0.8\n",
            "Row 2 - Zero-Shot Correctness Score: 0.27995988726615906\n",
            "Row 2 - Zero-Shot Completeness Score: 0.45225250720977783\n",
            "Row 2 - Zero-Shot Comprehensiveness Score: 0.33088964223861694\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 2 - Few-Shot Correctness Score: 0.8\n",
            "Row 2 - Few-Shot Completeness Score: 0.8\n",
            "Row 2 - Few-Shot Comprehensiveness Score: 0.8\n",
            "Row 3 - Zero-Shot Correctness Score: 0.3069274425506592\n",
            "Row 3 - Zero-Shot Completeness Score: 0.4567423462867737\n",
            "Row 3 - Zero-Shot Comprehensiveness Score: 0.36996200680732727\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 3 - Few-Shot Correctness Score: 0.8\n",
            "Row 3 - Few-Shot Completeness Score: 0.8\n",
            "Row 3 - Few-Shot Comprehensiveness Score: 0.8\n",
            "Row 4 - Zero-Shot Correctness Score: 0.2944944500923157\n",
            "Row 4 - Zero-Shot Completeness Score: 0.4603099524974823\n",
            "Row 4 - Zero-Shot Comprehensiveness Score: 0.37586668133735657\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 4 - Few-Shot Correctness Score: 0.8\n",
            "Row 4 - Few-Shot Completeness Score: 0.8\n",
            "Row 4 - Few-Shot Comprehensiveness Score: 0.8\n",
            "Row 5 - Zero-Shot Correctness Score: 0.2944944500923157\n",
            "Row 5 - Zero-Shot Completeness Score: 0.4603099524974823\n",
            "Row 5 - Zero-Shot Comprehensiveness Score: 0.37586668133735657\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 5 - Few-Shot Correctness Score: 0.8\n",
            "Row 5 - Few-Shot Completeness Score: 0.8\n",
            "Row 5 - Few-Shot Comprehensiveness Score: 0.8\n",
            "Row 6 - Zero-Shot Correctness Score: 0.303938090801239\n",
            "Row 6 - Zero-Shot Completeness Score: 0.45283010601997375\n",
            "Row 6 - Zero-Shot Comprehensiveness Score: 0.35967233777046204\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 6 - Few-Shot Correctness Score: 0.8\n",
            "Row 6 - Few-Shot Completeness Score: 0.8\n",
            "Row 6 - Few-Shot Comprehensiveness Score: 0.8\n",
            "Row 7 - Zero-Shot Correctness Score: 0.29617834091186523\n",
            "Row 7 - Zero-Shot Completeness Score: 0.44243043661117554\n",
            "Row 7 - Zero-Shot Comprehensiveness Score: 0.3510487377643585\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 7 - Few-Shot Correctness Score: 0.8\n",
            "Row 7 - Few-Shot Completeness Score: 0.8\n",
            "Row 7 - Few-Shot Comprehensiveness Score: 0.8\n",
            "\n",
            "Zero-Shot Average Correctness Score: 0.2963532507419586\n",
            "Zero-Shot Average Completeness Score: 0.451462413583483\n",
            "Zero-Shot Average Comprehensiveness Score: 0.3559497126511165\n",
            "\n",
            "Few-Shot Average Correctness Score: 0.7999999999999999\n",
            "Few-Shot Average Completeness Score: 0.7999999999999999\n",
            "Few-Shot Average Comprehensiveness Score: 0.7999999999999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Function to extract unique pairs of disease and description\n",
        "def get_unique_pairs(df):\n",
        "    unique_pairs = set()  # Store unique pairs in a set to avoid duplicates\n",
        "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing Entries\"):\n",
        "        pair = (row['label'], row['text'])\n",
        "        unique_pairs.add(pair)\n",
        "    return unique_pairs\n",
        "\n",
        "# Function to calculate uniqueness score for a dataset\n",
        "def calculate_uniqueness_score(df):\n",
        "    unique_pairs = get_unique_pairs(df)\n",
        "    total_entries = len(df)\n",
        "    unique_pairs_count = len(unique_pairs)\n",
        "    uniqueness_score = unique_pairs_count / total_entries\n",
        "    return uniqueness_score\n",
        "\n",
        "# Load the dataset\n",
        "dataset_path = 'output_file.csv'\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "# Calculate and print the uniqueness score for the dataset\n",
        "uniqueness_score = calculate_uniqueness_score(df)\n",
        "print(f\"Uniqueness Score {uniqueness_score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3m7yBZuc0pjI",
        "outputId": "f25090a7-92ef-42c3-d9f5-754641043efb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Entries: 100%|██████████| 23516/23516 [00:02<00:00, 8381.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uniqueness Score 0.8886\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "24dyb3Lc4WZs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}