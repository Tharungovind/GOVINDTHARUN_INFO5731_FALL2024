{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 664
        },
        "id": "fWZUKpfPkVwH",
        "outputId": "d75871b0-b772-41f5-b422-326b5c1efe38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1 (https://huggingface.co/facebook/bart-large-mnli).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "unsupported operand type(s) for +: 'int' and 'str'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-6259f61796a9>\u001b[0m in \u001b[0;36m<cell line: 111>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0mavg_zero_shot_comprehensiveness\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzero_shot_comprehensiveness_scores\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzero_shot_comprehensiveness_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m \u001b[0mavg_few_shot_correctness\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfew_shot_correctness_scores\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfew_shot_correctness_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0mavg_few_shot_completeness\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfew_shot_completeness_scores\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfew_shot_completeness_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0mavg_few_shot_comprehensiveness\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfew_shot_comprehensiveness_scores\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfew_shot_comprehensiveness_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'int' and 'str'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# Load the Hugging Face pipeline for Zero-Shot classification\n",
        "zero_shot_classifier = pipeline(\"zero-shot-classification\")\n",
        "\n",
        "# Function to get Hugging Face response for Zero-Shot classification\n",
        "def get_huggingface_response(prompt, model=\"facebook/bart-large-mnli\", temperature=0.7):\n",
        "    result = zero_shot_classifier(prompt, candidate_labels=[\"correct\", \"incorrect\", \"complete\", \"incomplete\", \"comprehensive\", \"incomplete\"])\n",
        "    return result['scores'][0]  # Return the score for the most likely label\n",
        "\n",
        "# Function to compute correctness score using Hugging Face (Zero-Shot and Few-Shot)\n",
        "def compute_correctness_score(description, disease_term, model=\"facebook/bart-large-mnli\"):\n",
        "    # Zero-shot Prompt: Hugging Face should evaluate how similar the disease term is to the description\n",
        "    zero_shot_prompt = f\"Evaluate the correctness of the disease description in relation to the disease term. The description: '{description}' and the disease term: '{disease_term}'. Rate the correctness on a scale from 0 to 1.\"\n",
        "\n",
        "    correctness_score_zero_shot = get_huggingface_response(zero_shot_prompt, model)\n",
        "    return float(correctness_score_zero_shot)  # Ensure the score is a float\n",
        "\n",
        "# Function to compute completeness score using Hugging Face (Zero-Shot and Few-Shot)\n",
        "def compute_completeness_score(description, model=\"facebook/bart-large-mnli\"):\n",
        "    # Zero-shot Prompt: Hugging Face should evaluate if the description covers all aspects of the disease\n",
        "    zero_shot_prompt = f\"Is the following disease description complete? Does it include key symptoms, diagnostic information, and relevant details about the disease? Describe any missing aspects. The description: '{description}'\"\n",
        "\n",
        "    completeness_score_zero_shot = get_huggingface_response(zero_shot_prompt, model)\n",
        "    return float(completeness_score_zero_shot)  # Ensure the score is a float\n",
        "\n",
        "# Function to compute comprehensiveness score using Hugging Face (Zero-Shot and Few-Shot)\n",
        "def compute_comprehensiveness_score(description, model=\"facebook/bart-large-mnli\"):\n",
        "    # Zero-shot Prompt: Hugging Face should evaluate the comprehensiveness of the description\n",
        "    zero_shot_prompt = f\"Is the following disease description comprehensive? Does it include symptoms, diagnostic tests, treatments, and all relevant details? Rate the comprehensiveness on a scale from 0 to 1. The description: '{description}'\"\n",
        "\n",
        "    comprehensiveness_score_zero_shot = get_huggingface_response(zero_shot_prompt, model)\n",
        "    return float(comprehensiveness_score_zero_shot)  # Ensure the score is a float\n",
        "\n",
        "# Few-shot Prompting Example for Correctness, Completeness, and Comprehensiveness\n",
        "def few_shot_prompt(prompt, model=\"gpt2\"):\n",
        "    # Example 1: Correctness\n",
        "    example_1 = \"Description: 'A condition characterized by joint pain and stiffness. Often causes inflammation.' Disease Term: 'Arthritis'. Correctness: 0.8\"\n",
        "    example_2 = \"Description: 'A disorder affecting the nervous system with symptoms of paralysis and muscle weakness.' Disease Term: 'Multiple Sclerosis'. Correctness: 0.9\"\n",
        "\n",
        "    # Example 2: Completeness\n",
        "    example_3 = \"Description: 'A disease caused by the flu virus, characterized by fever, chills, sore throat, and fatigue.' Completeness: 0.8\"\n",
        "    example_4 = \"Description: 'A chronic condition involving high blood sugar levels and insulin resistance.' Completeness: 0.9\"\n",
        "\n",
        "    # Example 3: Comprehensiveness\n",
        "    example_5 = \"Description: 'A condition that involves an autoimmune response that attacks the joints.' Comprehensiveness: 0.7\"\n",
        "    example_6 = \"Description: 'A mental health condition characterized by persistent sadness, feelings of hopelessness, and a lack of interest in daily activities.' Comprehensiveness: 0.85\"\n",
        "\n",
        "    # Combine the examples and prompt\n",
        "    few_shot_examples = \"\\n\".join([example_1, example_2, example_3, example_4, example_5, example_6])\n",
        "\n",
        "    prompt_with_few_shot = f\"{few_shot_examples}\\n\\n{prompt}\"\n",
        "\n",
        "    # Use the GPT-2 model from Hugging Face for Few-Shot Prompting\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model)\n",
        "\n",
        "    inputs = tokenizer(prompt_with_few_shot, return_tensors=\"pt\")\n",
        "    outputs = model.generate(**inputs, max_length=500, num_return_sequences=1, temperature=0.7)\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "# Read the CSV file using pandas (replace 'your_file.csv' with the actual file path)\n",
        "csv_file_path = 'MedMentions_Dataset.csv'  # Replace this with your actual CSV file path\n",
        "df = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Initialize score accumulators for both Zero-Shot and Few-Shot approaches\n",
        "zero_shot_correctness_scores = []\n",
        "zero_shot_completeness_scores = []\n",
        "zero_shot_comprehensiveness_scores = []\n",
        "\n",
        "few_shot_correctness_scores = []\n",
        "few_shot_completeness_scores = []\n",
        "few_shot_comprehensiveness_scores = []\n",
        "\n",
        "# Evaluate the first few rows of the CSV file\n",
        "for i in range(5):  # You can modify this to analyze more rows\n",
        "    description = df.iloc[i]['MentionTextSegment']  # Adjust the column name accordingly\n",
        "    disease_term = df.iloc[i]['MatchingContent']  # Adjust the column name accordingly\n",
        "\n",
        "    # Compute the scores using Zero-Shot approach\n",
        "    correctness_score_zero_shot = compute_correctness_score(description, disease_term)\n",
        "    completeness_score_zero_shot = compute_completeness_score(description)\n",
        "    comprehensiveness_score_zero_shot = compute_comprehensiveness_score(description)\n",
        "\n",
        "    zero_shot_correctness_scores.append(correctness_score_zero_shot)\n",
        "    zero_shot_completeness_scores.append(completeness_score_zero_shot)\n",
        "    zero_shot_comprehensiveness_scores.append(comprehensiveness_score_zero_shot)\n",
        "\n",
        "    # Compute the scores using Few-Shot approach\n",
        "    few_shot_correctness_prompt = f\"Evaluate the correctness of the disease description: '{description}' with the disease term: '{disease_term}'. Rate the correctness from 0 to 1.\"\n",
        "    few_shot_completeness_prompt = f\"Is the following disease description complete? Rate the completeness from 0 to 1. The description: '{description}'\"\n",
        "    few_shot_comprehensiveness_prompt = f\"Evaluate if the description includes symptoms, diagnostic tests, and treatments. Rate the comprehensiveness from 0 to 1. The description: '{description}'\"\n",
        "\n",
        "    correctness_score_few_shot = few_shot_prompt(few_shot_correctness_prompt)\n",
        "    completeness_score_few_shot = few_shot_prompt(few_shot_completeness_prompt)\n",
        "    comprehensiveness_score_few_shot = few_shot_prompt(few_shot_comprehensiveness_prompt)\n",
        "\n",
        "    few_shot_correctness_scores.append(correctness_score_few_shot)\n",
        "    few_shot_completeness_scores.append(completeness_score_few_shot)\n",
        "    few_shot_comprehensiveness_scores.append(comprehensiveness_score_few_shot)\n",
        "\n",
        "# Calculate the average scores for Zero-Shot and Few-Shot approaches\n",
        "avg_zero_shot_correctness = sum(zero_shot_correctness_scores) / len(zero_shot_correctness_scores)\n",
        "avg_zero_shot_completeness = sum(zero_shot_completeness_scores) / len(zero_shot_completeness_scores)\n",
        "avg_zero_shot_comprehensiveness = sum(zero_shot_comprehensiveness_scores) / len(zero_shot_comprehensiveness_scores)\n",
        "\n",
        "avg_few_shot_correctness = sum(few_shot_correctness_scores) / len(few_shot_correctness_scores)\n",
        "avg_few_shot_completeness = sum(few_shot_completeness_scores) / len(few_shot_completeness_scores)\n",
        "avg_few_shot_comprehensiveness = sum(few_shot_comprehensiveness_scores) / len(few_shot_comprehensiveness_scores)\n",
        "\n",
        "# Print out the average scores for both Zero-Shot and Few-Shot approaches\n",
        "print(f\"Average Zero-Shot Correctness Score: {avg_zero_shot_correctness}\")\n",
        "print(f\"Average Zero-Shot Completeness Score: {avg_zero_shot_completeness}\")\n",
        "print(f\"Average Zero-Shot Comprehensiveness Score: {avg_zero_shot_comprehensiveness}\")\n",
        "\n",
        "print(f\"Average Few-Shot Correctness Score: {avg_few_shot_correctness}\")\n",
        "print(f\"Average Few-Shot Completeness Score: {avg_few_shot_completeness}\")\n",
        "print(f\"Average Few-Shot Comprehensiveness Score: {avg_few_shot_comprehensiveness}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import re\n",
        "\n",
        "# Load the Hugging Face pipeline for Zero-Shot classification\n",
        "zero_shot_classifier = pipeline(\"zero-shot-classification\")\n",
        "\n",
        "# Function to get Hugging Face response for Zero-Shot classification\n",
        "def get_huggingface_response(prompt, model=\"facebook/bart-large-mnli\", temperature=0.7):\n",
        "    result = zero_shot_classifier(prompt, candidate_labels=[\"correct\", \"incorrect\", \"complete\", \"incomplete\", \"comprehensive\", \"incomplete\"])\n",
        "    return result['scores'][0]  # Return the score for the most likely label\n",
        "\n",
        "# Function to compute correctness score using Hugging Face (Zero-Shot and Few-Shot)\n",
        "def compute_correctness_score(description, disease_term, model=\"facebook/bart-large-mnli\"):\n",
        "    # Zero-shot Prompt: Hugging Face should evaluate how similar the disease term is to the description\n",
        "    zero_shot_prompt = f\"Evaluate the correctness of the disease description in relation to the disease term. The description: '{description}' and the disease term: '{disease_term}'. Rate the correctness on a scale from 0 to 1.\"\n",
        "\n",
        "    correctness_score_zero_shot = get_huggingface_response(zero_shot_prompt, model)\n",
        "    return correctness_score_zero_shot\n",
        "\n",
        "\n",
        "# Function to compute completeness score using Hugging Face (Zero-Shot and Few-Shot)\n",
        "def compute_completeness_score(description, model=\"facebook/bart-large-mnli\"):\n",
        "    # Zero-shot Prompt: Hugging Face should evaluate if the description covers all aspects of the disease\n",
        "    zero_shot_prompt = f\"Is the following disease description complete? Does it include key symptoms, diagnostic information, and relevant details about the disease? Describe any missing aspects. The description: '{description}'\"\n",
        "\n",
        "    completeness_score_zero_shot = get_huggingface_response(zero_shot_prompt, model)\n",
        "    return completeness_score_zero_shot\n",
        "\n",
        "\n",
        "# Function to compute comprehensiveness score using Hugging Face (Zero-Shot and Few-Shot)\n",
        "def compute_comprehensiveness_score(description, model=\"facebook/bart-large-mnli\"):\n",
        "    # Zero-shot Prompt: Hugging Face should evaluate the comprehensiveness of the description\n",
        "    zero_shot_prompt = f\"Is the following disease description comprehensive? Does it include symptoms, diagnostic tests, treatments, and all relevant details? Rate the comprehensiveness on a scale from 0 to 1. The description: '{description}'\"\n",
        "\n",
        "    comprehensiveness_score_zero_shot = get_huggingface_response(zero_shot_prompt, model)\n",
        "    return comprehensiveness_score_zero_shot\n",
        "\n",
        "\n",
        "# Few-shot Prompting Example for Correctness, Completeness, and Comprehensiveness\n",
        "def few_shot_prompt(prompt, model=\"gpt2\"):\n",
        "    # Example 1: Correctness\n",
        "    example_1 = \"Description: 'A condition characterized by joint pain and stiffness. Often causes inflammation.' Disease Term: 'Arthritis'. Correctness: 0.8\"\n",
        "    example_2 = \"Description: 'A disorder affecting the nervous system with symptoms of paralysis and muscle weakness.' Disease Term: 'Multiple Sclerosis'. Correctness: 0.9\"\n",
        "\n",
        "    # Example 2: Completeness\n",
        "    example_3 = \"Description: 'A disease caused by the flu virus, characterized by fever, chills, sore throat, and fatigue.' Completeness: 0.8\"\n",
        "    example_4 = \"Description: 'A chronic condition involving high blood sugar levels and insulin resistance.' Completeness: 0.9\"\n",
        "\n",
        "    # Example 3: Comprehensiveness\n",
        "    example_5 = \"Description: 'A condition that involves an autoimmune response that attacks the joints.' Comprehensiveness: 0.7\"\n",
        "    example_6 = \"Description: 'A mental health condition characterized by persistent sadness, feelings of hopelessness, and a lack of interest in daily activities.' Comprehensiveness: 0.85\"\n",
        "\n",
        "    # Combine the examples and prompt\n",
        "    few_shot_examples = \"\\n\".join([example_1, example_2, example_3, example_4, example_5, example_6])\n",
        "\n",
        "    prompt_with_few_shot = f\"{few_shot_examples}\\n\\n{prompt}\"\n",
        "\n",
        "    # Use the GPT-2 model from Hugging Face for Few-Shot Prompting\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model)\n",
        "\n",
        "    inputs = tokenizer(prompt_with_few_shot, return_tensors=\"pt\")\n",
        "    outputs = model.generate(**inputs, max_length=400, num_return_sequences=1, temperature=0.7)\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract the numerical score from the response\n",
        "    match = re.search(r\"(\\d+\\.\\d+)\", response)\n",
        "    if match:\n",
        "        return float(match.group(1))  # Return the extracted score\n",
        "    else:\n",
        "        return 0.0  # Return a default score in case no score is found\n",
        "\n",
        "\n",
        "# Read the CSV file using pandas (replace 'your_file.csv' with the actual file path)\n",
        "csv_file_path = 'MedMentions_Dataset.csv'  # Replace this with your actual CSV file path\n",
        "df = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Initialize variables to accumulate scores for averaging\n",
        "zero_shot_correctness_total = 0\n",
        "zero_shot_completeness_total = 0\n",
        "zero_shot_comprehensiveness_total = 0\n",
        "\n",
        "few_shot_correctness_total = 0\n",
        "few_shot_completeness_total = 0\n",
        "few_shot_comprehensiveness_total = 0\n",
        "\n",
        "# Evaluate the first few rows of the CSV file\n",
        "num_rows = 7\n",
        "for i in range(num_rows):\n",
        "    description = df.iloc[i]['MentionTextSegment']  # Adjust the column name accordingly\n",
        "    disease_term = df.iloc[i]['MatchingContent']  # Adjust the column name accordingly\n",
        "\n",
        "    # Compute the scores using Zero-Shot and Few-Shot approaches\n",
        "\n",
        "    # Zero-Shot Approach\n",
        "    correctness_score_zero_shot = compute_correctness_score(description, disease_term)\n",
        "    completeness_score_zero_shot = compute_completeness_score(description)\n",
        "    comprehensiveness_score_zero_shot = compute_comprehensiveness_score(description)\n",
        "\n",
        "    zero_shot_correctness_total += correctness_score_zero_shot\n",
        "    zero_shot_completeness_total += completeness_score_zero_shot\n",
        "    zero_shot_comprehensiveness_total += comprehensiveness_score_zero_shot\n",
        "\n",
        "    print(f\"Row {i+1} - Zero-Shot Correctness Score: {correctness_score_zero_shot}\")\n",
        "    print(f\"Row {i+1} - Zero-Shot Completeness Score: {completeness_score_zero_shot}\")\n",
        "    print(f\"Row {i+1} - Zero-Shot Comprehensiveness Score: {comprehensiveness_score_zero_shot}\")\n",
        "\n",
        "    # Few-Shot Approach\n",
        "    few_shot_correctness_prompt = f\"Evaluate the correctness of the disease description: '{description}' with the disease term: '{disease_term}'. Rate the correctness from 0 to 1.\"\n",
        "    few_shot_completeness_prompt = f\"Is the following disease description complete? Rate the completeness from 0 to 1. The description: '{description}'\"\n",
        "    few_shot_comprehensiveness_prompt = f\"Evaluate if the description includes symptoms, diagnostic tests, and treatments. Rate the comprehensiveness from 0 to 1. The description: '{description}'\"\n",
        "\n",
        "    correctness_score_few_shot = few_shot_prompt(few_shot_correctness_prompt)\n",
        "    completeness_score_few_shot = few_shot_prompt(few_shot_completeness_prompt)\n",
        "    comprehensiveness_score_few_shot = few_shot_prompt(few_shot_comprehensiveness_prompt)\n",
        "\n",
        "    few_shot_correctness_total += correctness_score_few_shot\n",
        "    few_shot_completeness_total += completeness_score_few_shot\n",
        "    few_shot_comprehensiveness_total += comprehensiveness_score_few_shot\n",
        "\n",
        "    print(f\"Row {i+1} - Few-Shot Correctness Score: {correctness_score_few_shot}\")\n",
        "    print(f\"Row {i+1} - Few-Shot Completeness Score: {completeness_score_few_shot}\")\n",
        "    print(f\"Row {i+1} - Few-Shot Comprehensiveness Score: {comprehensiveness_score_few_shot}\")\n",
        "\n",
        "# Calculate and print the average scores for Zero-Shot and Few-Shot\n",
        "zero_shot_correctness_avg = zero_shot_correctness_total / num_rows\n",
        "zero_shot_completeness_avg = zero_shot_completeness_total / num_rows\n",
        "zero_shot_comprehensiveness_avg = zero_shot_comprehensiveness_total / num_rows\n",
        "\n",
        "few_shot_correctness_avg = few_shot_correctness_total / num_rows\n",
        "few_shot_completeness_avg = few_shot_completeness_total / num_rows\n",
        "few_shot_comprehensiveness_avg = few_shot_comprehensiveness_total / num_rows\n",
        "\n",
        "print(f\"\\nZero-Shot Average Correctness Score: {zero_shot_correctness_avg}\")\n",
        "print(f\"Zero-Shot Average Completeness Score: {zero_shot_completeness_avg}\")\n",
        "print(f\"Zero-Shot Average Comprehensiveness Score: {zero_shot_comprehensiveness_avg}\")\n",
        "\n",
        "print(f\"\\nFew-Shot Average Correctness Score: {few_shot_correctness_avg}\")\n",
        "print(f\"Few-Shot Average Completeness Score: {few_shot_completeness_avg}\")\n",
        "print(f\"Few-Shot Average Comprehensiveness Score: {few_shot_comprehensiveness_avg}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83S5JX9ynXhT",
        "outputId": "fce8d663-a519-4bcf-e429-7f02c8600a43"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1 (https://huggingface.co/facebook/bart-large-mnli).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 1 - Zero-Shot Correctness Score: 0.3037741184234619\n",
            "Row 1 - Zero-Shot Completeness Score: 0.4333570897579193\n",
            "Row 1 - Zero-Shot Comprehensiveness Score: 0.3177729547023773\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 1 - Few-Shot Correctness Score: 0.8\n",
            "Row 1 - Few-Shot Completeness Score: 0.8\n",
            "Row 1 - Few-Shot Comprehensiveness Score: 0.8\n",
            "Row 2 - Zero-Shot Correctness Score: 0.29976484179496765\n",
            "Row 2 - Zero-Shot Completeness Score: 0.4333570897579193\n",
            "Row 2 - Zero-Shot Comprehensiveness Score: 0.3177729547023773\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 2 - Few-Shot Correctness Score: 0.8\n",
            "Row 2 - Few-Shot Completeness Score: 0.8\n",
            "Row 2 - Few-Shot Comprehensiveness Score: 0.8\n",
            "Row 3 - Zero-Shot Correctness Score: 0.3162427246570587\n",
            "Row 3 - Zero-Shot Completeness Score: 0.4322035610675812\n",
            "Row 3 - Zero-Shot Comprehensiveness Score: 0.3128163516521454\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 3 - Few-Shot Correctness Score: 0.8\n",
            "Row 3 - Few-Shot Completeness Score: 0.8\n",
            "Row 3 - Few-Shot Comprehensiveness Score: 0.8\n",
            "Row 4 - Zero-Shot Correctness Score: 0.31633275747299194\n",
            "Row 4 - Zero-Shot Completeness Score: 0.4317966401576996\n",
            "Row 4 - Zero-Shot Comprehensiveness Score: 0.3159773647785187\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 4 - Few-Shot Correctness Score: 0.8\n",
            "Row 4 - Few-Shot Completeness Score: 0.8\n",
            "Row 4 - Few-Shot Comprehensiveness Score: 0.8\n",
            "Row 5 - Zero-Shot Correctness Score: 0.34930673241615295\n",
            "Row 5 - Zero-Shot Completeness Score: 0.4254850149154663\n",
            "Row 5 - Zero-Shot Comprehensiveness Score: 0.318319708108902\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 5 - Few-Shot Correctness Score: 0.8\n",
            "Row 5 - Few-Shot Completeness Score: 0.8\n",
            "Row 5 - Few-Shot Comprehensiveness Score: 0.8\n",
            "Row 6 - Zero-Shot Correctness Score: 0.31633275747299194\n",
            "Row 6 - Zero-Shot Completeness Score: 0.4317966401576996\n",
            "Row 6 - Zero-Shot Comprehensiveness Score: 0.3159773647785187\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 6 - Few-Shot Correctness Score: 0.8\n",
            "Row 6 - Few-Shot Completeness Score: 0.8\n",
            "Row 6 - Few-Shot Comprehensiveness Score: 0.8\n",
            "Row 7 - Zero-Shot Correctness Score: 0.318731427192688\n",
            "Row 7 - Zero-Shot Completeness Score: 0.4337688386440277\n",
            "Row 7 - Zero-Shot Comprehensiveness Score: 0.3141212463378906\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 7 - Few-Shot Correctness Score: 0.8\n",
            "Row 7 - Few-Shot Completeness Score: 0.8\n",
            "Row 7 - Few-Shot Comprehensiveness Score: 0.8\n",
            "\n",
            "Zero-Shot Average Correctness Score: 0.31721219420433044\n",
            "Zero-Shot Average Completeness Score: 0.43168069635118755\n",
            "Zero-Shot Average Comprehensiveness Score: 0.3161082778658186\n",
            "\n",
            "Few-Shot Average Correctness Score: 0.7999999999999999\n",
            "Few-Shot Average Completeness Score: 0.7999999999999999\n",
            "Few-Shot Average Comprehensiveness Score: 0.7999999999999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Function to extract unique pairs of disease and description\n",
        "def get_unique_pairs(df):\n",
        "    unique_pairs = set()  # Store unique pairs in a set to avoid duplicates\n",
        "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing Entries\"):\n",
        "        pair = (row['MatchingContent'], row['MentionTextSegment'])\n",
        "        unique_pairs.add(pair)\n",
        "    return unique_pairs\n",
        "\n",
        "# Function to calculate uniqueness score for a dataset\n",
        "def calculate_uniqueness_score(df):\n",
        "    unique_pairs = get_unique_pairs(df)\n",
        "    total_entries = len(df)\n",
        "    unique_pairs_count = len(unique_pairs)\n",
        "    uniqueness_score = unique_pairs_count / total_entries\n",
        "    return uniqueness_score\n",
        "\n",
        "# Load the dataset\n",
        "dataset_path = 'MedMentions_Dataset.csv'\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "# Calculate and print the uniqueness score for the dataset\n",
        "uniqueness_score = calculate_uniqueness_score(df)\n",
        "print(f\"Uniqueness Score {uniqueness_score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdHEFQEe1TTu",
        "outputId": "48502c73-9268-47ad-e2d2-da187db226b8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Entries: 100%|██████████| 382320/382320 [00:28<00:00, 13479.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uniqueness Score 0.2947\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HB4e9tr93sUh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}