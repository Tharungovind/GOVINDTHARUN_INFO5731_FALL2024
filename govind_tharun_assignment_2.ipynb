{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tharungovind/GOVINDTHARUN_INFO5731_FALL2024/blob/main/govind_tharun_assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 2**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: Tuesday, at 11:59 PM.\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n",
        "\n",
        "**Please check that the link you submitted can be opened and points to the correct assignment.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (40 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "(3) Collect all the reviews of the top 1000 most popular software from G2 or Capterra.\n",
        "\n",
        "(4) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the Densho Digital Repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jDyTKYs-yGit",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43620cfd-e95f-4077-b422-700a1ee66ee3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting reviews for Oppenheimer...\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "Collected 1000 reviews for Oppenheimer\n",
            "Oppenheimer: 1000 reviews\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "\n",
        "def get_reviews(movie_id, max_reviews=1000):\n",
        "    reviews = []\n",
        "    page = 0\n",
        "    headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
        "    }\n",
        "    while len(reviews) < max_reviews:\n",
        "        url = f\"https://www.imdb.com/title/{movie_id}/reviews/_ajax?paginationKey=\"\n",
        "        response = requests.get(url,headers=headers)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        review_blocks = soup.find_all('div', class_='text show-more__control')\n",
        "\n",
        "        for review_block in review_blocks:\n",
        "            reviews.append(review_block.get_text())\n",
        "            if len(reviews) % 100 == 0 :\n",
        "               print(len(reviews))\n",
        "\n",
        "            if len(reviews) >= max_reviews:\n",
        "                break\n",
        "\n",
        "        pagination_key = soup.find('div', {'data-key': True})\n",
        "\n",
        "        if pagination_key:\n",
        "            page = pagination_key['data-key']\n",
        "        else:\n",
        "            break\n",
        "\n",
        "        time.sleep(2)\n",
        "\n",
        "    return reviews\n",
        "\n",
        "movies = [\n",
        "    {\"title\": \"Oppenheimer\", \"id\": \"tt15398776\"},\n",
        "]\n",
        "\n",
        "all_reviews = {}\n",
        "\n",
        "for movie in movies:\n",
        "    print(f\"Collecting reviews for {movie['title']}...\")\n",
        "    reviews = get_reviews(movie['id'], max_reviews=1000)\n",
        "    all_reviews[movie['title']] = reviews\n",
        "    print(f\"Collected {len(reviews)} reviews for {movie['title']}\")\n",
        "\n",
        "for movie, reviews in all_reviews.items():\n",
        "    print(f\"{movie}: {len(reviews)} reviews\")\n",
        "\n",
        "import csv\n",
        "\n",
        "for movie, reviews in all_reviews.items():\n",
        "    with open(f\"{movie}_reviews.csv\", 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(['Review'])\n",
        "        for review in reviews:\n",
        "            writer.writerow([review])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (30 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5QX6bJjGWXY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "94555ce7-354b-49db-fbe7-bba489d85b5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# Write code for each of the sub parts with proper comments.\n",
        "!pip install pandas nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv('/content/Oppenheimer_reviews.csv')\n",
        "\n",
        "print(\"Columns in the DataFrame:\", df.columns)\n",
        "print(df.head())\n",
        "\n",
        "# Strip whitespace from column names\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return ''\n",
        "\n",
        "    # (1) Remove noise (special characters and punctuations)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # (2) Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # (3) Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = ' '.join(word for word in text.split() if word.lower() not in stop_words)\n",
        "\n",
        "    # (4) Lowercase all texts\n",
        "    text = text.lower()\n",
        "\n",
        "    # (5) Stemming\n",
        "    ps = PorterStemmer()\n",
        "    text = ' '.join(ps.stem(word) for word in text.split())\n",
        "\n",
        "    # (6) Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    text = ' '.join(lemmatizer.lemmatize(word) for word in text.split())\n",
        "\n",
        "    return text\n",
        "\n",
        "df['cleaned_text'] = df['Review'].apply(clean_text)\n",
        "\n",
        "df.to_csv('cleaned_data.csv', index=False)\n",
        "print(\"Data cleaned and saved successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siMyMN-RxPDn",
        "outputId": "51f91273-fdc6-48e9-f3ca-e754c95383b6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in the DataFrame: Index(['Review'], dtype='object')\n",
            "                                              Review\n",
            "0  You'll have to have your wits about you and yo...\n",
            "1  One of the most anticipated films of the year ...\n",
            "2  I'm a big fan of Nolan's work so was really lo...\n",
            "3  I'm still collecting my thoughts after experie...\n",
            "4  \"Oppenheimer\" is a biographical thriller film ...\n",
            "Data cleaned and saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (30 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Y0oOSlsOS0cq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "a1bcc026-ac0c-4f36-eae6-b556c2e8bf96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Example Sentence: youll wit brain fulli switch watch oppenheim could easili get away nonattent viewer intellig filmmak show audienc great respect fire dialogu pack inform relentless pace jump differ time oppenheim life continu hour runtim visual clue guid viewer time youll get grip quit quickli relentless help express urgenc u attack chase atom bomb germani could absolut career best perform consistenli brilliant cillian murphi anchor film nail oscar perform fact whole cast fantast apart mayb sometim overwrought emili blunt perform rdj also particularli brilliant return proper act decad call screenplay den layer id say thick bibl cinematographi quit stark spare part imbu rich luciou colour moment especi scene florenc pugh score beauti time mostli anxiou oppress ad relentless pace hour runtim fli found intens tax highli reward watch film make finest realli great watch\n",
            "Dependency Parsing:\n",
            "you -> nsubj (Head: wit)\n",
            "ll -> aux (Head: wit)\n",
            "wit -> nsubj (Head: absolut)\n",
            "brain -> compound (Head: switch)\n",
            "fulli -> compound (Head: switch)\n",
            "switch -> compound (Head: watch)\n",
            "watch -> dobj (Head: wit)\n",
            "oppenheim -> nsubj (Head: easili)\n",
            "could -> aux (Head: easili)\n",
            "easili -> ccomp (Head: wit)\n",
            "get -> conj (Head: wit)\n",
            "away -> prt (Head: get)\n",
            "nonattent -> compound (Head: filmmak)\n",
            "viewer -> compound (Head: intellig)\n",
            "intellig -> compound (Head: filmmak)\n",
            "filmmak -> nsubj (Head: show)\n",
            "show -> conj (Head: wit)\n",
            "audienc -> nmod (Head: fire)\n",
            "great -> amod (Head: respect)\n",
            "respect -> compound (Head: fire)\n",
            "fire -> nmod (Head: inform)\n",
            "dialogu -> amod (Head: pack)\n",
            "pack -> nsubj (Head: inform)\n",
            "inform -> conj (Head: wit)\n",
            "relentless -> amod (Head: jump)\n",
            "pace -> compound (Head: jump)\n",
            "jump -> dobj (Head: inform)\n",
            "differ -> conj (Head: wit)\n",
            "time -> dobj (Head: differ)\n",
            "oppenheim -> compound (Head: life)\n",
            "life -> compound (Head: continu)\n",
            "continu -> nmod (Head: clue)\n",
            "hour -> nmod (Head: clue)\n",
            "runtim -> nmod (Head: clue)\n",
            "visual -> amod (Head: clue)\n",
            "clue -> compound (Head: guid)\n",
            "guid -> appos (Head: time)\n",
            "viewer -> amod (Head: time)\n",
            "time -> dobj (Head: wit)\n",
            "you -> nsubj (Head: get)\n",
            "ll -> aux (Head: get)\n",
            "get -> relcl (Head: time)\n",
            "grip -> nsubj (Head: quit)\n",
            "quit -> ccomp (Head: wit)\n",
            "quickli -> dobj (Head: quit)\n",
            "relentless -> amod (Head: help)\n",
            "help -> aux (Head: express)\n",
            "express -> conj (Head: wit)\n",
            "urgenc -> compound (Head: germani)\n",
            "u -> compound (Head: chase)\n",
            "attack -> compound (Head: chase)\n",
            "chase -> compound (Head: germani)\n",
            "atom -> compound (Head: bomb)\n",
            "bomb -> compound (Head: germani)\n",
            "germani -> dobj (Head: express)\n",
            "could -> aux (Head: absolut)\n",
            "absolut -> ROOT (Head: absolut)\n",
            "career -> nmod (Head: oscar)\n",
            "best -> advmod (Head: career)\n",
            "perform -> compound (Head: brilliant)\n",
            "consistenli -> compound (Head: brilliant)\n",
            "brilliant -> compound (Head: oscar)\n",
            "cillian -> compound (Head: murphi)\n",
            "murphi -> compound (Head: oscar)\n",
            "anchor -> compound (Head: oscar)\n",
            "film -> compound (Head: oscar)\n",
            "nail -> compound (Head: oscar)\n",
            "oscar -> nsubj (Head: perform)\n",
            "perform -> ccomp (Head: absolut)\n",
            "fact -> dobj (Head: perform)\n",
            "whole -> amod (Head: fantast)\n",
            "cast -> compound (Head: fantast)\n",
            "fantast -> npadvmod (Head: perform)\n",
            "apart -> prep (Head: perform)\n",
            "mayb -> compound (Head: emili)\n",
            "sometim -> nmod (Head: emili)\n",
            "overwrought -> compound (Head: emili)\n",
            "emili -> compound (Head: perform)\n",
            "blunt -> compound (Head: perform)\n",
            "perform -> compound (Head: rdj)\n",
            "rdj -> conj (Head: perform)\n",
            "also -> advmod (Head: perform)\n",
            "particularli -> amod (Head: return)\n",
            "brilliant -> amod (Head: return)\n",
            "return -> dobj (Head: perform)\n",
            "proper -> amod (Head: act)\n",
            "act -> compound (Head: layer)\n",
            "decad -> compound (Head: call)\n",
            "call -> compound (Head: layer)\n",
            "screenplay -> compound (Head: layer)\n",
            "den -> compound (Head: layer)\n",
            "layer -> appos (Head: return)\n",
            "i -> nsubj (Head: say)\n",
            "d -> nsubj (Head: say)\n",
            "say -> ROOT (Head: say)\n",
            "thick -> amod (Head: cinematographi)\n",
            "bibl -> compound (Head: cinematographi)\n",
            "cinematographi -> nsubj (Head: quit)\n",
            "quit -> ccomp (Head: say)\n",
            "stark -> amod (Head: moment)\n",
            "spare -> amod (Head: part)\n",
            "part -> nmod (Head: imbu)\n",
            "imbu -> nmod (Head: luciou)\n",
            "rich -> amod (Head: luciou)\n",
            "luciou -> compound (Head: moment)\n",
            "colour -> compound (Head: moment)\n",
            "moment -> dobj (Head: quit)\n",
            "especi -> ccomp (Head: quit)\n",
            "scene -> compound (Head: pugh)\n",
            "florenc -> compound (Head: pugh)\n",
            "pugh -> compound (Head: beauti)\n",
            "score -> compound (Head: beauti)\n",
            "beauti -> compound (Head: time)\n",
            "time -> npadvmod (Head: especi)\n",
            "mostli -> nsubj (Head: anxiou)\n",
            "anxiou -> relcl (Head: time)\n",
            "oppress -> compound (Head: ad)\n",
            "ad -> nmod (Head: hour)\n",
            "relentless -> amod (Head: pace)\n",
            "pace -> compound (Head: hour)\n",
            "hour -> dobj (Head: anxiou)\n",
            "runtim -> amod (Head: fli)\n",
            "fli -> nsubj (Head: found)\n",
            "found -> conj (Head: say)\n",
            "intens -> dobj (Head: found)\n",
            "tax -> compound (Head: highli)\n",
            "highli -> compound (Head: reward)\n",
            "reward -> compound (Head: film)\n",
            "watch -> compound (Head: film)\n",
            "film -> nsubj (Head: make)\n",
            "make -> ccomp (Head: found)\n",
            "finest -> amod (Head: watch)\n",
            "realli -> nmod (Head: watch)\n",
            "great -> amod (Head: watch)\n",
            "watch -> dobj (Head: make)\n",
            "\n",
            "Constituency Parsing:\n",
            "you (PRON) --> nsubj\n",
            "ll (AUX) --> aux\n",
            "wit (NOUN) --> nsubj\n",
            "brain (NOUN) --> compound\n",
            "fulli (PROPN) --> compound\n",
            "switch (PROPN) --> compound\n",
            "watch (PROPN) --> dobj\n",
            "oppenheim (PROPN) --> nsubj\n",
            "could (AUX) --> aux\n",
            "easili (VERB) --> ccomp\n",
            "get (VERB) --> conj\n",
            "away (ADP) --> prt\n",
            "nonattent (PROPN) --> compound\n",
            "viewer (PROPN) --> compound\n",
            "intellig (PROPN) --> compound\n",
            "filmmak (PROPN) --> nsubj\n",
            "show (VERB) --> conj\n",
            "audienc (ADJ) --> nmod\n",
            "great (ADJ) --> amod\n",
            "respect (NOUN) --> compound\n",
            "fire (NOUN) --> nmod\n",
            "dialogu (ADJ) --> amod\n",
            "pack (NOUN) --> nsubj\n",
            "inform (VERB) --> conj\n",
            "relentless (ADJ) --> amod\n",
            "pace (NOUN) --> compound\n",
            "jump (NOUN) --> dobj\n",
            "differ (VERB) --> conj\n",
            "time (NOUN) --> dobj\n",
            "oppenheim (PROPN) --> compound\n",
            "life (NOUN) --> compound\n",
            "continu (NOUN) --> nmod\n",
            "hour (NOUN) --> nmod\n",
            "runtim (ADJ) --> nmod\n",
            "visual (ADJ) --> amod\n",
            "clue (NOUN) --> compound\n",
            "guid (PROPN) --> appos\n",
            "viewer (ADJ) --> amod\n",
            "time (NOUN) --> dobj\n",
            "you (PRON) --> nsubj\n",
            "ll (AUX) --> aux\n",
            "get (VERB) --> relcl\n",
            "grip (NOUN) --> nsubj\n",
            "quit (VERB) --> ccomp\n",
            "quickli (PROPN) --> dobj\n",
            "relentless (ADJ) --> amod\n",
            "help (AUX) --> aux\n",
            "express (VERB) --> conj\n",
            "urgenc (ADJ) --> compound\n",
            "u (PROPN) --> compound\n",
            "attack (NOUN) --> compound\n",
            "chase (PROPN) --> compound\n",
            "atom (PROPN) --> compound\n",
            "bomb (PROPN) --> compound\n",
            "germani (PROPN) --> dobj\n",
            "could (AUX) --> aux\n",
            "absolut (VERB) --> ROOT\n",
            "career (NOUN) --> nmod\n",
            "best (ADJ) --> advmod\n",
            "perform (PROPN) --> compound\n",
            "consistenli (PROPN) --> compound\n",
            "brilliant (PROPN) --> compound\n",
            "cillian (PROPN) --> compound\n",
            "murphi (PROPN) --> compound\n",
            "anchor (PROPN) --> compound\n",
            "film (NOUN) --> compound\n",
            "nail (NOUN) --> compound\n",
            "oscar (PROPN) --> nsubj\n",
            "perform (VERB) --> ccomp\n",
            "fact (NOUN) --> dobj\n",
            "whole (ADJ) --> amod\n",
            "cast (VERB) --> compound\n",
            "fantast (NOUN) --> npadvmod\n",
            "apart (ADP) --> prep\n",
            "mayb (PROPN) --> compound\n",
            "sometim (PROPN) --> nmod\n",
            "overwrought (PROPN) --> compound\n",
            "emili (PROPN) --> compound\n",
            "blunt (NOUN) --> compound\n",
            "perform (NOUN) --> compound\n",
            "rdj (PROPN) --> conj\n",
            "also (ADV) --> advmod\n",
            "particularli (ADJ) --> amod\n",
            "brilliant (ADJ) --> amod\n",
            "return (NOUN) --> dobj\n",
            "proper (ADJ) --> amod\n",
            "act (NOUN) --> compound\n",
            "decad (PROPN) --> compound\n",
            "call (PROPN) --> compound\n",
            "screenplay (NOUN) --> compound\n",
            "den (PROPN) --> compound\n",
            "layer (NOUN) --> appos\n",
            "i (PROPN) --> nsubj\n",
            "d (PROPN) --> nsubj\n",
            "say (VERB) --> ROOT\n",
            "thick (PROPN) --> amod\n",
            "bibl (PROPN) --> compound\n",
            "cinematographi (PROPN) --> nsubj\n",
            "quit (VERB) --> ccomp\n",
            "stark (ADJ) --> amod\n",
            "spare (ADJ) --> amod\n",
            "part (NOUN) --> nmod\n",
            "imbu (NOUN) --> nmod\n",
            "rich (ADJ) --> amod\n",
            "luciou (PROPN) --> compound\n",
            "colour (NOUN) --> compound\n",
            "moment (NOUN) --> dobj\n",
            "especi (VERB) --> ccomp\n",
            "scene (NOUN) --> compound\n",
            "florenc (PROPN) --> compound\n",
            "pugh (NOUN) --> compound\n",
            "score (NOUN) --> compound\n",
            "beauti (PROPN) --> compound\n",
            "time (NOUN) --> npadvmod\n",
            "mostli (PROPN) --> nsubj\n",
            "anxiou (VERB) --> relcl\n",
            "oppress (NOUN) --> compound\n",
            "ad (NOUN) --> nmod\n",
            "relentless (ADJ) --> amod\n",
            "pace (NOUN) --> compound\n",
            "hour (NOUN) --> dobj\n",
            "runtim (ADJ) --> amod\n",
            "fli (NOUN) --> nsubj\n",
            "found (VERB) --> conj\n",
            "intens (NOUN) --> dobj\n",
            "tax (NOUN) --> compound\n",
            "highli (PROPN) --> compound\n",
            "reward (NOUN) --> compound\n",
            "watch (NOUN) --> compound\n",
            "film (NOUN) --> nsubj\n",
            "make (VERB) --> ccomp\n",
            "finest (ADJ) --> amod\n",
            "realli (PROPN) --> nmod\n",
            "great (ADJ) --> amod\n",
            "watch (NOUN) --> dobj\n",
            "\n",
            "Named Entity Recognition Counts:\n",
            "PERSON: 4200\n",
            "ORG: 2160\n",
            "NORP: 1040\n",
            "CARDINAL: 1760\n",
            "DATE: 480\n",
            "TIME: 360\n",
            "ORDINAL: 680\n",
            "GPE: 1560\n",
            "EVENT: 80\n",
            "WORK_OF_ART: 80\n",
            "FAC: 120\n",
            "LOC: 80\n",
            "PRODUCT: 120\n",
            "Syntax and structure analysis saved to 'syntax_structure_analysis.csv'\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "import spacy\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "# Load the spacy model for English language processing\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "df = pd.read_csv('/content/cleaned_data.csv')\n",
        "\n",
        "def pos_tag(text):\n",
        "    doc = nlp(text)\n",
        "    pos_counts = Counter([token.pos_ for token in doc])\n",
        "\n",
        "    # Count nouns, verbs, adjectives, and adverbs\n",
        "    noun_count = pos_counts.get('NOUN', 0)\n",
        "    verb_count = pos_counts.get('VERB', 0)\n",
        "    adj_count = pos_counts.get('ADJ', 0)\n",
        "    adv_count = pos_counts.get('ADV', 0)\n",
        "\n",
        "    return noun_count, verb_count, adj_count, adv_count\n",
        "\n",
        "df['Nouns'], df['Verbs'], df['Adjectives'], df['Adverbs'] = zip(*df['cleaned_text'].apply(pos_tag))\n",
        "\n",
        "def parsing_analysis(text):\n",
        "    doc = nlp(text)\n",
        "\n",
        "    print(\"Dependency Parsing:\")\n",
        "    for token in doc:\n",
        "        print(f'{token.text} -> {token.dep_} (Head: {token.head.text})')\n",
        "\n",
        "    print(\"\\nConstituency Parsing:\")\n",
        "    for token in doc:\n",
        "        print(f'{token.text} ({token.pos_}) --> {token.dep_}')\n",
        "\n",
        "example_sentence = df['cleaned_text'].iloc[0]\n",
        "print(f\"\\nExample Sentence: {example_sentence}\")\n",
        "parsing_analysis(example_sentence)\n",
        "\n",
        "def extract_named_entities(text):\n",
        "    doc = nlp(text)\n",
        "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "    return entities\n",
        "\n",
        "df['Entities'] = df['cleaned_text'].apply(extract_named_entities)\n",
        "\n",
        "entity_counts = Counter()\n",
        "for entities in df['Entities']:\n",
        "    for ent in entities:\n",
        "        entity_counts[ent[1]] += 1\n",
        "\n",
        "print(\"\\nNamed Entity Recognition Counts:\")\n",
        "for entity_type, count in entity_counts.items():\n",
        "    print(f'{entity_type}: {count}')\n",
        "\n",
        "df.to_csv('syntax_structure_analysis.csv', index=False)\n",
        "print(\"Syntax and structure analysis saved to 'syntax_structure_analysis.csv'\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Comment**\n",
        "Make sure to submit the cleaned data CSV in the comment section - 10 points"
      ],
      "metadata": {
        "id": "CXNn1lEVbMsv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "/content/syntax_structure_analysis.csv"
      ],
      "metadata": {
        "id": "qYRO5Cn8bYwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ],
      "metadata": {
        "id": "q8BFCvWp32cf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your response below\n",
        "\n",
        "#I really enjoyed in executing the code for this assignment as it involved from Web scraping to Text feature . I got some difficulties in executing the code i went to lectures and able to execute the code. the time provided for execution of code is not sufficient for me as extension of time helped for me to complete thhe assignment"
      ],
      "metadata": {
        "id": "_e557s2w4BpK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "OP4-5qxAtInv"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}